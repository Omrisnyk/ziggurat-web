(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{QUfp:function(e,t,a){"use strict";a.r(t),a.d(t,"_frontmatter",(function(){return i})),a.d(t,"default",(function(){return m}));var n=a("wx14"),s=a("zLVn"),o=(a("q1tI"),a("7ljp")),r=a("hhGP"),i=(a("qKvR"),{});void 0!==i&&i&&i===Object(i)&&Object.isExtensible(i)&&!i.hasOwnProperty("__filemeta")&&Object.defineProperty(i,"__filemeta",{configurable:!0,value:{name:"_frontmatter",filename:"docs/concepts.mdx"}});var c={_frontmatter:i},u=r.a;function m(e){var t=e.components,a=Object(s.a)(e,["components"]);return Object(o.b)(u,Object(n.a)({},c,a,{components:t,mdxType:"MDXLayout"}),Object(o.b)("h1",{id:"concepts"},"Concepts"),Object(o.b)("hr",null),Object(o.b)("p",null,"Stream Routes"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Stream routes are analogous to HTTP routes in the sense that instead of an API call, messages from a Kafka stream are routed to a handler function.\nIn Ziggurat, we use Kafka streams to get messages from the particular topic (or a set of topics). Ziggurat supports reading\ndata from multiple streams and since each stream can possibly route messages to a different handler-function the concept\nof stream routes act as a really significant abstraction in the framework.")),Object(o.b)("h2",{id:"retrying-messages"},"Retrying Messages"),Object(o.b)("p",null,"Please refer the ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"/docs/retries-and-queues"}),"Retries and Queues")," document for this."),Object(o.b)("p",null,"Actor Routes"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Ziggurat starts an HTTP server to provide APIs for ping health check and viewing, retrying messages in the deadset queues.\nIt also enables the user to pass in their own routes so that they don't have to set up their own server. The ",Object(o.b)("a",{href:"https://github.com/gojek/ziggurat/blob/master/README.md",target:"_blank",rel:"noopener"}," Readme ")," has details on how to pass in the actor routes.")),Object(o.b)("p",null,"Channels"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"The maximum number of threads that we can run for reading from kafka is the number of partitions that the topic has. Further details are explained\n",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://docs.confluent.io/current/streams/architecture.html"}),"here"),". This is an excerpt from the document")),Object(o.b)("pre",null,Object(o.b)("code",Object(n.a)({parentName:"pre"},{}),"Slightly simplified, the maximum parallelism at which your application may run is bounded by the maximum number of stream\ntasks, which itself is determined by maximum number of partitions of the input topic(s) the application is reading from.\nFor example, if your input topic has 5 partitions, then you can run up to 5 applications instances. These instances will\ncollaboratively process the topic’s data. If you run a larger number of app instances than partitions of the input topic,\nthe “excess” app instances will launch but remain idle; however, if one of the busy instances goes down, one of the idle\n; instances will resume the former’s work. We provide a more detailed explanation and example in the FAQ.\n")),Object(o.b)("p",null,"Ziggurat is a stream processing framework, and in a lot of our use cases, it is necessary for us to keep a minimum consumer lag.\nBut we faced issues where the mapper-function execution time was too high and we could not increase the number of parallel consumers\ndue to the constraint set by the number of partitions. So we introduced the concept of channels. A channel sets up an exchange\nand queues in Rabbitmq and pushes messages directly to Rabbitmq from Kafka (without applying the mapper-function). Consumers\nthen read the messages from the Rabbitmq queues and since we can scale up the consumers on Rabbitmq indefinitely we can keep\nthe consumer lag in check.\nLet us understand this with an example. Assume that we consume from a topic, flash, that produces message at 10 messages/sec and has 2 partitions.\nTo keep the consumer lag from getting out of hand our mapper-function can take a maximum processing time of 200 ms\n(2 consumers, so every consumer has to read 5 messages per second). But due to an external API call the execution time goes up\nto 500 ms. Thus we can consume only 4 messages/sec now. To fix this the consumers can read messages from Kafka and directly put\nthem into Rabbitmq. Then we can have 5 or 6 consumers on Rabbitmq that read the messages and process them. That is all that\nchannels do. "))}void 0!==m&&m&&m===Object(m)&&Object.isExtensible(m)&&!m.hasOwnProperty("__filemeta")&&Object.defineProperty(m,"__filemeta",{configurable:!0,value:{name:"MDXContent",filename:"docs/concepts.mdx"}}),m.isMDXComponent=!0}}]);
//# sourceMappingURL=component---docs-concepts-mdx-d058978c70ae3e1fa6ca.js.map